{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43fccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import os, cv2\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from skimage.util import random_noise\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb45cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 15\n",
    "NOISE_FACTOR = 0.2\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 256, 256\n",
    "BATCH_SIZE = 16\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ab4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définitions des répertoire \n",
    "PATH_MODEL_SAVE = './PATH_MODEL_SAVE'\n",
    "PATH_DATASET ='../Dataset2'\n",
    "PATH_GAUSSIAN_BLURRED = './Blurred_j'\n",
    "PATH_GAUSSIAN_NOISED ='./Noisy_j'\n",
    "\n",
    "if not os.path.exists(PATH_GAUSSIAN_BLURRED):\n",
    "    os.mkdirs(PATH_GAUSSIAN_BLURRED)\n",
    "    \n",
    "if not os.path.exists(PATH_GAUSSIAN_NOISED):\n",
    "    os.mkdirs(PATH_GAUSSIAN_NOISED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53c16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui va afficher nos images \n",
    "def display_image(X, n):\n",
    "    plt.figure(figsize = (32, 32))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i+1)\n",
    "        plt.imshow(X[i])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbc2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blur dataset ready !\n"
     ]
    }
   ],
   "source": [
    "# Bruitage des images avec du bruit gaussien\n",
    "images = os.listdir(PATH_DATASET)\n",
    "def create_blurred_folder():\n",
    "    for i, img in tqdm(enumerate(images), total = len(images)):\n",
    "        img = cv2.imread(f\"{PATH_DATASET}/{images[i]}\")\n",
    "        blur = cv2.GaussianBlur(img, (51, 51), 0)\n",
    "        cv2.imwrite(f\"{PATH_GAUSSIAN_BLURRED}/{images[i]}\", blur)\n",
    "\n",
    "if len(os.listdir(PATH_GAUSSIAN_BLURRED)) < len(images):\n",
    "    create_blurred_folder()\n",
    "else:\n",
    "    print('Blur dataset ready !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7d570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_images():\n",
    "    for image_path in os.listdir(PATH_DATASET):\n",
    "        input_path = os.path.join(PATH_DATASET, image_path).replace(\"\\\\\", \"/\")\n",
    "        img_to_noise = cv2.imread(input_path)\n",
    "        noisy_image = random_noise(img_to_noise, mode='gaussian')\n",
    "        noisy_image = np.array(255*noisy_image, dtype = 'uint8')\n",
    "        cv2.imwrite(PATH_GAUSSIAN_NOISED + image_path, noisy_image)\n",
    "        \n",
    "if len(os.listdir(PATH_GAUSSIAN_NOISED)) < len(images):\n",
    "    create_noisy_images()\n",
    "else:\n",
    "    print('Noisy dataset ready !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a822f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 0 classes.\n",
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory ../Dataset2. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/4042628789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m dataset_train = tf.keras.preprocessing.image_dataset_from_directory (\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mPATH_DATASET\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimage_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_WIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m       image_paths, labels, validation_split, subset)\n\u001b[0;32m    206\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     raise ValueError(f'No images found in directory {directory}. '\n\u001b[0m\u001b[0;32m    208\u001b[0m                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory ../Dataset2. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "dataset_train = tf.keras.preprocessing.image_dataset_from_directory (\n",
    "    PATH_DATASET,\n",
    "    image_size = (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    subset = 'training',\n",
    "    color_mode = \"rgb\"\n",
    ")\n",
    "\n",
    "x_train, _ = iter(dataset_train).next()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "dataset_test = tf.keras.preprocessing.image_dataset_from_directory (\n",
    "    PATH_DATASET,\n",
    "    image_size = (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    subset = 'validation',\n",
    "    color_mode = \"rgb\"\n",
    ")\n",
    "\n",
    "x_test, _ = iter(dataset_test).next()\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = np.reshape(x_test, (len(x_test), IMAGE_WIDTH, IMAGE_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde7a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création des jeux de données bruités\n",
    "dataset_train_noisy = tf.keras.preprocessing.image_dataset_from_directory (\n",
    "    PATH_GAUSSIAN_NOISED ,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = 'int',\n",
    "    image_size = (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    subset = 'training',\n",
    "    color_mode = \"rgb\"\n",
    ")\n",
    "\n",
    "x_train_noisy, _ = iter(dataset_train_noisy).next()\n",
    "x_train_noisy = x_train_noisy.astype('float32') / 255.\n",
    "x_train_noisy = np.reshape(x_train_noisy, (len(x_train_noisy), IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "dataset_test_noisy = tf.keras.preprocessing.image_dataset_from_directory (\n",
    "    PATH_GAUSSIAN_NOISED,\n",
    "    labels = \"inferred\",\n",
    "    label_mode = 'int',\n",
    "    image_size = (IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 42,\n",
    "    subset = 'validation',\n",
    "    color_mode = \"rgb\"\n",
    ")\n",
    "\n",
    "x_test_noisy, _ = iter(dataset_test_noisy).next()\n",
    "x_test_noisy = x_test_noisy.astype('float32') / 255.\n",
    "x_test_noisy = np.reshape(x_test_noisy, (len(x_test_noisy), IMAGE_WIDTH, IMAGE_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c243c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des jeux de test et validation\n",
    "\n",
    "x_train_noisy = x_train + np.random.normal(loc = 0, scale = 1, size = x_train.shape)*NOISE_FACTOR\n",
    "x_validation_noisy = x_test + np.random.normal(loc=0, scale = 1, size = x_test.shape)*NOISE_FACTOR\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_validation_noisy = np.clip(x_validation_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les images normales \n",
    "display_image(x_train, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On afficher les images bruitées \n",
    "display_image(x_train_noisy, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c59d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "INPUT_SHAPE = Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "# ENCODING\n",
    "# Convolution 1\n",
    "x = Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name=\"conv_1_encoder\")(INPUT_SHAPE)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\", name=\"pooling_1\")(x)\n",
    "\n",
    "# Convolution 2\n",
    "x = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', name=\"conv_2_encoder\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\", name=\"pooling_2\")(x)\n",
    "\n",
    "# Convolution 3\n",
    "x = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', name=\"conv_3_encoder\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\", name=\"pooling_3\")(x)\n",
    "\n",
    "# Convolution 4\n",
    "x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', name=\"conv_4_encoder\")(x)\n",
    "\n",
    "encode = MaxPooling2D((2,2), padding=\"same\", name=\"pooling_4\")(x)\n",
    "\n",
    "# DECODING\n",
    "# Déconvolution 1\n",
    "x = Conv2DTranspose(filters=256, kernel_size=3, strides=2, padding='same', activation='relu', name=\"conv_1_decoder\")(encode)\n",
    "#x = UpSampling2D((2, 2), name=\"sampling_1\")(x)\n",
    "\n",
    "# Déconvolution 2\n",
    "x = Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"conv_2_decoder\")(x)\n",
    "#x = UpSampling2D((2, 2), name=\"sampling_2\")(x)\n",
    "\n",
    "# Déconvolution 3\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"conv_3_decoder\")(x)\n",
    "#x = UpSampling2D((2, 2), name=\"sampling_3\")(x)\n",
    "\n",
    "# Déconvolution 4\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"conv_4_decoder\")(x)\n",
    "#x = UpSampling2D((2, 2), name=\"sampling_4\")(x)\n",
    " \n",
    "# Output    \n",
    "decode = Conv2D(3, (3, 3),  padding=\"same\", activation=\"sigmoid\", name=\"output\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition de l'autoencoder et compilation du modèle \n",
    "autoencoder = Model(INPUT_SHAPE, decode, name=\"autoencoder\")\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On entraine l'autoencoder\n",
    "history = autoencoder.fit(x_train_noisy, x_train, epochs= EPOCHS, batch_size = BATCH_SIZE, \n",
    "                          steps_per_epoch = STEPS_PER_EPOCH, shuffle = True, validation_data = (x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adab6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les courbes loss du train et test \n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b76385",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(x_test_noisy)\n",
    "display_image(x_test, n=5)\n",
    "display_image(x_test_noisy, n=5)\n",
    "display_image(predictions, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef90e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation = autoencoder.evaluate(dataset_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2533968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde le modèle pour le réutiliser\n",
    "autoencoder.save('PATH_MODEL_SAVE')\n",
    "model = tf.keras.models.load_model(PATH_MODEL_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met l'image de test dans le bon format\n",
    "input_img = PIL.Image.open('D:\\CESI\\A5\\Data\\Datasets\\Test\\images2.jpg').convert('RGB')\n",
    "input_img = input_img.resize((256, 256), PIL.Image.ANTIALIAS)\n",
    "input_img = np.array(input_img, dtype=np.float32)\n",
    "input_img = input_img.astype('float32')/255\n",
    "input_img = tf.expand_dims(input_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire une version débruité\n",
    "denoised = model.predict(input_img)\n",
    "display_image(denoised, n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
