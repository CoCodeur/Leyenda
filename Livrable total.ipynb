{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/kagglesdsdata/datasets/1867533/3049878/photo_livrable/Logo_cesi.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20220116%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220116T124656Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=27e1d22de5054393a37acebf2096d04f072fa784e9509b67da9bce59b858af2206789716335caf988de23e14fae97c1daa800b718a0781becc6621176569bce4f399752f3b66ba66b8dca1ad337afd9c8a2c876063992d84e8a22bac1fdaf61e5423b9bac68c5c474a131f031f8f2bd55fcbef4d9a6c2deef0564dbdcf67d839080d7ac4c4aab4239a25ceed70f286bd2a6ef6fb2f06e386daf57dd1525c9f13fc2b00c2cd4ba5b9eb3d326232b80518cf49318b48ed9b2533ac548f92c2ef0ab56fad3ce3248bddbc5287e48d115ad1775e010b13a56666229105308ec9735e24d45d83a07b5e316ce5dc2af4de87965690f25ba7c6b6c2e99f9cbef306e66f)\n",
    "\n",
    "# Projet Leyenda\n",
    "## Livrable général\n",
    "|Auteurs|\n",
    "|---|\n",
    "|Karim Salhi\n",
    "|Corentin Devrouete\n",
    "|Hugo Larose\n",
    "|Pierre-Alain Tietz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce livrable est de regrouper les 3 livrables du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des librairies\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = pathlib.Path(\"./datasets_test\")\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def livrable1(img, model):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    \n",
    "    img  = image.img_to_array(img)\n",
    "    img  = img.reshape((1,) + img.shape)\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "\n",
    "    score = prediction[0][0]\n",
    "    other_score = (1 - score)\n",
    "\n",
    "    formatted_photo_score = 100 * score\n",
    "    formatted_other_score = 100 * other_score\n",
    "\n",
    "    score_array = [score,other_score]\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.grid(True)\n",
    "    plt.bar([\"Photo\",\"autre\"], score_array)\n",
    "    #plt.xticks(range(NB_CLASSES), [\"Photo\",\"other\"], rotation = 45)\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Cette image est %.2f %% une photo et %.2f %% autre.\" % (score*100, other_score*100))\n",
    "    return score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def livrable2(img, model):\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_test = np.reshape(x_test, (len(x_test), IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "    x_test_noisy = x_test + np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) * NOISE_FACTOR\n",
    "\n",
    "    x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "    predictions = model.predict(x_test_noisy)\n",
    "    display_image(x_test, 5)\n",
    "    display_image(x_test_noisy, 5)\n",
    "    display_image(predictions, 5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def livrable3(img):\n",
    "    # Chemin du fichier d'annotations\n",
    "    annotation_folder = \"../input/ms-coco/annotations_trainval2014/annotations/\"\n",
    "    annotation_file = annotation_folder+\"captions_train2014.json\"\n",
    "\n",
    "    # Chemin du dossier contenant les images à annoter\n",
    "    image_folder = \"../input/ms-coco/train2014/train2014/\"\n",
    "    PATH = image_folder\n",
    "\n",
    "\n",
    "    # Lecture du fichier d'annotation\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Grouper toutes les annotations ayant le meme identifiant.\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "    for val in annotations['annotations']:\n",
    "        # marquer le debut et la fin de chaque annotation\n",
    "        caption = f\"<start> {val['caption']} <end>\"\n",
    "        # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "        image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "        # Rajout du caption associé à image_path\n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "    # Prendre les 2000 premières images seulement\n",
    "    image_paths = list(image_path_to_caption.keys())\n",
    "    train_image_paths = image_paths[:2000]\n",
    "\n",
    "    # Liste de toutes les annotations\n",
    "    train_captions = []\n",
    "    # Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "    img_name_vector = []\n",
    "\n",
    "    for image_path in train_image_paths:\n",
    "        caption_list = image_path_to_caption[image_path]\n",
    "        # Rajout de caption_list dans train_captions\n",
    "        train_captions.extend(caption_list)\n",
    "        # Rajout de image_path dupliquée len(caption_list) fois\n",
    "        img_name_vector.extend([image_path] * len(caption_list))\n",
    "        \n",
    "    # Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                    weights='imagenet')\n",
    "    # Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "    new_input = image_model.input \n",
    "    # récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    # Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "    # Définition de la fonction load_image\n",
    "    def load_image(image_path):\n",
    "        \"\"\"\n",
    "        La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "        contenant l'image traitée ainsi que son chemin d'accès.\n",
    "        La fonction load_image effectue les traitement suivant:\n",
    "            1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "            2. Décodage de l'image en RGB.\n",
    "            3. Redimensionnement de l'image en taille (299, 299).\n",
    "            4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "        \"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path\n",
    "\n",
    "    # Pré-traitement des images\n",
    "    # Prendre les noms des images\n",
    "    encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "    # Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "    # Division du données en batchs après application du pré-traitement fait par load_image\n",
    "    image_dataset = image_dataset.map(\n",
    "      load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "    \n",
    "    # Trouver la taille maximale \n",
    "    def calc_max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "    # Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "    top_k = 5000\n",
    "    #La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                      oov_token=\"<unk>\",\n",
    "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    # Construit un vocabulaire en se basant sur la liste train_captions\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "    # Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "    # Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "    # Remplir chaque vecteur jusqu'à la longueur maximale des annotations\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "    # Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "    # Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "    max_length = calc_max_length(train_seqs)\n",
    "\n",
    "    img_to_cap_vector = collections.defaultdict(list)\n",
    "    # Creation d'un dictionnaire associant les chemins des images avec (fichier .npy) aux annotationss\n",
    "    # Les images sont dupliquées car il y a plusieurs annotations par image\n",
    "    for img, cap in zip(img_name_vector, cap_vector):\n",
    "        img_to_cap_vector[img].append(cap)\n",
    "\n",
    "    \"\"\"\n",
    "    Création des datasets de formation et de validation en utilisant \n",
    "    un fractionnement 80-20 de manière aléatoire\n",
    "    \"\"\" \n",
    "    # Prendre les clés (noms des fichiers d'images traites), *celles-ci ne seront pas dupliquées*\n",
    "    img_keys = list(img_to_cap_vector.keys())\n",
    "    random.shuffle(img_keys)\n",
    "    # Diviser des indices en entrainement et test\n",
    "    slice_index = int(len(img_keys)*0.8)\n",
    "    img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "    \"\"\"\n",
    "    Les jeux d'entrainement et de tests sont sous forme\n",
    "    de listes contenants les mappings :(image prétraitée ---> jeton d'annotation(mot) )\n",
    "    \"\"\"\n",
    "\n",
    "    # Boucle pour construire le jeu d'entrainement\n",
    "    img_name_train = []\n",
    "    cap_train = []\n",
    "    for imgt in img_name_train_keys:\n",
    "        capt_len = len(img_to_cap_vector[imgt])\n",
    "        # Duplication des images en le nombre d'annotations par image\n",
    "        img_name_train.extend([imgt] * capt_len)\n",
    "        cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "    # Boucle pour construire le jeu de test\n",
    "    img_name_val = []\n",
    "    cap_val = []\n",
    "    for imgv in img_name_val_keys:\n",
    "        capv_len =len(img_to_cap_vector[imgv]) \\\n",
    "        # Duplication des images en le nombre d'annotations par image\n",
    "        img_name_val.extend([imgv] * capv_len)\n",
    "        cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "    # N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "    BATCH_SIZE = 64 # taille du batch\n",
    "    BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "    embedding_dim = 256\n",
    "    units = 512 # Taille de la couche caché dans le RNN\n",
    "    vocab_size = top_k + 1\n",
    "    num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "    # La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "    # Les deux variables suivantes representent la forme de ce vecteur\n",
    "    features_shape = 2048\n",
    "    attention_features_shape = 64\n",
    "\n",
    "    # Fonction qui charge les fichiers numpy des images prétraitées\n",
    "    def map_func(img_name, cap):\n",
    "        #img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "        img_tensor = np.load(\"../input/ms-coco/preprocessed_images/preprocessed_images/\" + os.path.split(img_name.decode('utf-8'))[-1] + '.npy')\n",
    "        return img_tensor, cap\n",
    "\n",
    "    # Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "    # Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "    # L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "              map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Melanger les donnees et les diviser en batchs\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    class CNN_Encoder(tf.keras.Model):\n",
    "        # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "        # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "        def __init__(self, embedding_dim):\n",
    "            super(CNN_Encoder, self).__init__()\n",
    "            # forme après fc == (batch_size, 64, embedding_dim)\n",
    "            self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "        def call(self, x):\n",
    "            x = self.fc(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            return x\n",
    "    \n",
    "    class BahdanauAttention(tf.keras.Model):\n",
    "        def __init__(self, units):\n",
    "            super(BahdanauAttention, self).__init__()\n",
    "            self.W1 = tf.keras.layers.Dense(units) \n",
    "            self.W2 = tf.keras.layers.Dense(units) \n",
    "            self.V = tf.keras.layers.Dense(1) \n",
    "\n",
    "        def call(self, features, hidden):\n",
    "            # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "            # forme de la couche cachée == (batch_size, hidden_size)\n",
    "            hidden_with_time_axis = tf.expand_dims(hidden, 1) \n",
    "\n",
    "            attention_hidden_layer = (tf.nn.tanh(self.W1(features)+self.W2(hidden_with_time_axis)))\n",
    "\n",
    "            # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "            score = self.V(attention_hidden_layer) \n",
    "\n",
    "            attention_weights = tf.nn.softmax(score, axis=1) \n",
    "\n",
    "            context_vector = attention_weights * features \n",
    "            context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "            return context_vector, attention_weights\n",
    "        \n",
    "    class RNN_Decoder(tf.keras.Model):\n",
    "        def __init__(self, embedding_dim, units, vocab_size):\n",
    "            super(RNN_Decoder, self).__init__()\n",
    "            self.units = units\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "            self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer='glorot_uniform')\n",
    "            #Couche dense qui aura pour entrée la sortie du GRU\n",
    "            self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "            # Dernière couche dense\n",
    "            self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "            self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "        def call(self, x, features, hidden):\n",
    "            # L'attention est defini par un modèle a part\n",
    "            context_vector, attention_weights = self.attention(features, hidden)\n",
    "            # Passage du mot courant à la couche embedding\n",
    "            x = self.embedding(x)\n",
    "            # Concaténation\n",
    "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "            # Passage du vecteur concaténé à la gru\n",
    "            output, state = self.gru(x)\n",
    "\n",
    "            # Couche dense\n",
    "            y = self.fc1(output)\n",
    "\n",
    "            y = tf.reshape(y, (-1, x.shape[2]))\n",
    "\n",
    "            # Couche dense\n",
    "            y = self.fc2(y)\n",
    "\n",
    "            return y, state, attention_weights\n",
    "\n",
    "        def reset_state(self, batch_size):\n",
    "            return tf.zeros((batch_size, self.units))\n",
    "    \n",
    "    # Création de l'encodeur\n",
    "    encoder = CNN_Encoder(embedding_dim)\n",
    "    # Création du décodeur\n",
    "    decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "    # Optimiseur ADAM\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    # La fonction de perte\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_mean(loss_)\n",
    "    \n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               optimizer = optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "        # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        \n",
    "    loss_plot = []\n",
    "    @tf.function\n",
    "    def train_step(img_tensor, target):\n",
    "        loss = 0\n",
    "\n",
    "        # Initialisation de l'état caché pour chaque batch\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "        # Initialiser l'entrée du décodeur\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "        with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "            features = encoder(img_tensor)\n",
    "\n",
    "            for i in range(1, target.shape[1]):\n",
    "                # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "                predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "                loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "                # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "                dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "    \n",
    "    EPOCHS = 50\n",
    "\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "        # sauvegarde de la perte\n",
    "        loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            ckpt_manager.save()\n",
    "\n",
    "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                             total_loss/num_steps))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    def evaluate(image):\n",
    "        attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "        hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "        temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "        img_tensor_val = image_features_extract_model(temp_input)\n",
    "        img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "        features = encoder(img_tensor_val)\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "        result = []\n",
    "\n",
    "        for i in range(max_length):\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "            predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "            result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "            if tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, attention_plot\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result), :]\n",
    "        return result, attention_plot\n",
    "\n",
    "    # Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "    def plot_attention(image, result, attention_plot):\n",
    "        temp_image = np.array(Image.open(image))\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "        len_result = len(result)\n",
    "        for l in range(len_result):\n",
    "            temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "            ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "            ax.set_title(result[l])\n",
    "            img = ax.imshow(temp_image)\n",
    "            ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    result2, attention_plot2 = evaluate('img')\n",
    "    return result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision =  true positives / (true positives + false positives)\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + kb.epsilon())\n",
    "    return recall\n",
    "\n",
    "# f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = kb.sum(kb.round(kb.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = kb.sum(kb.round(kb.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + kb.epsilon())\n",
    "    possible_positives = kb.sum(kb.round(kb.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + kb.epsilon())\n",
    "    f1_score = 2*(precision*recall)/(precision+recall+kb.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(dataset):\n",
    "    model_1 = keras.models.load_model('./model_livrable_1/cnn_real.h5', custom_objects={'f1_score':f1_score, 'precision':precision, 'recall':recall})\n",
    "    model_2 = tf.keras.models.load_model('./model_livrable_2_ameliore')\n",
    "    for pictures in os.listdir(dataset):\n",
    "        img = image.load_img((dataset + \"/\" + pictures), target_size = (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        plt.figure()\n",
    "        plt.imshow(img) \n",
    "        plt.show()\n",
    "        classification = livrable1(img, model_1)\n",
    "        if classification>50 :\n",
    "            img = livrable2(img, model_2)\n",
    "            description = livrable3(img)\n",
    "            print ('Annotation prédite:', ' '.join(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at model_livrable_1/cnn_real.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-46e3143f6185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_DATASET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-8053783834c0>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_livrable_1/cnn_real.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'f1_score'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'precision'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recall'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_livrable_2_ameliore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpictures\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpictures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_WIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             raise ImportError(\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at model_livrable_1/cnn_real.h5"
     ]
    }
   ],
   "source": [
    "pipeline(PATH_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
